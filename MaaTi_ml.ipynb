{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py\n",
    "# https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca\n",
    "# https://realpython.com/k-means-clustering-python/\n",
    "\n",
    "#-Huokostila%=(Ds-Db/Ds)*100\n",
    "#-WC10 = kenttäkapasiteetin (–10 kPa matriisipotentiaalissa) vesipit. til.%\n",
    "#-AFP10 =kenttäkapsiteetin ilmatila = kok.tilavuus-WC10 til.%\n",
    "#-WC1500 = lakastumisrajan vesipit.til.%\n",
    "\n",
    "# also add:\n",
    "# org+clay -ratio \n",
    "\n",
    "# think also should 100 kPa > 10 kPa and 0.3 < 1 kPa as the lowest and higest values are measured in different methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 125 entries, 24 to 238\n",
      "Data columns (total 22 columns):\n",
      "lat_N                  125 non-null int64\n",
      "lon_E                  125 non-null int64\n",
      "altitude               125 non-null int64\n",
      "soil_type              125 non-null category\n",
      "grain_size             125 non-null category\n",
      "orglayer_thickness     125 non-null float64\n",
      "topography             125 non-null category\n",
      "slope                  125 non-null int64\n",
      "hydraulic_regime       125 non-null category\n",
      "peat_forming_mosses    125 non-null float64\n",
      "sitetype               125 non-null category\n",
      "clay                   125 non-null float64\n",
      "silt                   125 non-null float64\n",
      "sand                   125 non-null float64\n",
      "gravel                 125 non-null float64\n",
      "dry_density            125 non-null float64\n",
      "bulk_density           125 non-null float64\n",
      "organic_content        125 non-null float64\n",
      "alpha_vg2              125 non-null float64\n",
      "n_vg2                  125 non-null float64\n",
      "theta-s                125 non-null float64\n",
      "theta-ratio            125 non-null float64\n",
      "dtypes: category(5), float64(13), int64(4)\n",
      "memory usage: 19.0 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:6702: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  regex=regex,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from MaaTi_io import categorical_vars\n",
    "\n",
    "# read data from csv-file\n",
    "raw_data = pd.read_csv('wrc-calculated_BioSoil_Maati.csv')\n",
    "raw_data.set_index(raw_data['site_id'])\n",
    "data = raw_data.dropna()\n",
    "\n",
    "# select columns used in clustering\n",
    "\n",
    "cols = [\n",
    "    #'depth',\n",
    "    'lat_N',  # continuous\n",
    "    'lon_E',  # continuos\n",
    "    'altitude',  # continuous\n",
    "    'soil_type',  # catergorical: 3: till; 4: sorted\n",
    "    'grain_size',  # categorical: 1=clay/silt (savi, hiesu, hieno hieta); 2=silt/sand (karkea hieta, hieno hiekka); 3=sand/gravel (karkea hiekka, sora)  \n",
    "    'orglayer_thickness',  # continuous\n",
    "    'topography',  # categorical: 0=flat; 1=hilltop/uphill; 2=hillside; 3=downhill; 4=hollow/depression; 5=other\n",
    "    'slope',  # continuous\n",
    "    'hydraulic_regime',  # categorical: 1=barren heath; 2=sub-xeric; 3=xeric; 4=mesic; 5=?; 6=hydric\n",
    "    'peat_forming_mosses',  # continuos\n",
    "    'sitetype',  # 1=herb-rich; 2=?; 3=mesic; 4=xeric; 5=subxeric; 6=barren heath; 7=rocky or sandy areas; 8=hilltops\n",
    "    'clay',\n",
    "    'silt',\n",
    "    'sand',\n",
    "    'gravel',\n",
    "    'dry_density',  # continuous\n",
    "    'bulk_density',  # continuous  \n",
    "    'organic_content'  # continuous\n",
    "]\n",
    "\n",
    "continuous_vars = [\n",
    "    #'depth',\n",
    "    'lat_N',  \n",
    "    'lon_E',\n",
    "    'altitude',\n",
    "    'orglayer_thickness',\n",
    "    'slope',\n",
    "    'peat_forming_mosses',\n",
    "    'clay',\n",
    "    'silt',\n",
    "    'sand',\n",
    "    'gravel',\n",
    "    'dry_density', \n",
    "    'bulk_density',\n",
    "    'organic_content'\n",
    "]\n",
    "\n",
    "# preprocess categorical variables\n",
    "data.replace(categorical_vars, inplace=True)\n",
    "for var in categorical_vars:\n",
    "    if var in data:\n",
    "        data[var] = data[var].astype(dtype='category')\n",
    "        \n",
    "vg2par = [\n",
    "    'alpha_vg2',\n",
    "    'n_vg2',\n",
    "    'theta-s',\n",
    "    'theta-ratio'\n",
    "]\n",
    "\n",
    "vg3par = [\n",
    "    'alpha_vg3',\n",
    "    'n_vg3',\n",
    "    'theta-r_vg3',\n",
    "    'theta-s',\n",
    "    'theta-ratio'\n",
    "]\n",
    "\n",
    "cols_vg2par = cols + vg2par\n",
    "cols_vg3par = cols + vg3par\n",
    "\n",
    "data_vg2 = data[cols_vg2par]\n",
    "data_vg3 = data[cols_vg3par]\n",
    "\n",
    "# row labels\n",
    "labels = data['site_id'].values\n",
    "\n",
    "data_vg2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\t\tinertia\tch-score\tdb-score\tsilhouette\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<class 'sklearn.preprocessing._encoders.OneHotEncoder'>' (type <class 'type'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c363cb99d247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'init\\t\\tinertia\\tch-score\\tdb-score\\tsilhouette'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mkmeans_biosoil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_vg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-c363cb99d247>\u001b[0m in \u001b[0;36mkmeans_biosoil\u001b[0;34m(n_clusters, name, data)\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcolumn_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     message=self._log_message(name, idx, len(transformers)))\n\u001b[1;32m    419\u001b[0m                 for idx, (name, trans, column, weight) in enumerate(\n\u001b[0;32m--> 420\u001b[0;31m                         self._iter(fitted=fitted, replace_strings=True), 1))\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected 2D array, got 1D array instead\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    752\u001b[0m             tasks = BatchedCalls(itertools.islice(iterator, batch_size),\n\u001b[1;32m    753\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nested_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                                  self._pickle_cache)\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                 \u001b[0;31m# No more tasks available in the iterator: tell caller to stop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterator_slice, backend_and_jobs, pickle_cache)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_and_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_and_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ColumnTransformer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     message=self._log_message(name, idx, len(transformers)))\n\u001b[0;32m--> 419\u001b[0;31m                 for idx, (name, trans, column, weight) in enumerate(\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self._iter(fitted=fitted, replace_strings=True), 1))\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     58\u001b[0m                             \u001b[0;34m\"it does not seem to be a scikit-learn estimator \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                             \u001b[0;34m\"as it does not implement a 'get_params' methods.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                             % (repr(estimator), type(estimator)))\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<class 'sklearn.preprocessing._encoders.OneHotEncoder'>' (type <class 'type'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods."
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def kmeans_biosoil(n_clusters, name, data):\n",
    "\n",
    "    column_transformer = ColumnTransformer(\n",
    "        [('category', OneHotEncoder, list(categorical_vars.keys())),\n",
    "        ('float', MinMaxScaler, continuous_vars)]\n",
    "    )\n",
    "    \n",
    "    column_transformer.fit_transform(data_vg2)\n",
    "    \n",
    "    pca = PCA(n_components=n_clusters).fit(data_vg2)\n",
    "    kmeans = KMeans(init=pca.components_, n_clusters=n_clusters, n_init=1)\n",
    "    \n",
    "    estimator = make_pipeline(PCA(), kmeans).fit()\n",
    "    \n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "    \n",
    "    clustering_metrics = [\n",
    "        metrics.calinski_harabasz_score,\n",
    "        metrics.davies_bouldin_score\n",
    "        #metrics.silhouette_score,\n",
    "    ]\n",
    "    \n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "    results += [\n",
    "        metrics.silhouette_score(data, estimator[-1].labels_,\n",
    "                                metric=\"euclidean\", sample_size=300,)\n",
    "    ]\n",
    "    \n",
    "    # Show the results\n",
    "    formatter_results = (\"{:9s}\\t{:.3f}s\\t{:.3f}\\t{:.3f}\")\n",
    "    print(formatter_results.format(*results))\n",
    "    \n",
    "\n",
    "print('init\\t\\tinertia\\tch-score\\tdb-score\\tsilhouette')\n",
    "for n_clusters in range(2, 11):\n",
    "    kmeans_biosoil(n_clusters, name=\"{}\".format(n_clusters), data=data_vg2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our evaluation benchmark\n",
    "# comparison of different initialization methods for KMeans\n",
    "\n",
    "# Benchmark will:\n",
    "# - create a pipeline which scale the data using a StandardScaler\n",
    "# - train and time the pipeline fitting\n",
    "# - measure the performance of the clustering obtained via different metrics\n",
    "\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def bench_kmeans(kmeans, name, data, labels):\n",
    "    \"\"\" Benchmark to evaluete the KMeans initialization methods.\n",
    "    \n",
    "    Args:\n",
    "        kmeans: KMeans instance\n",
    "        name: name given to the strategy\n",
    "        data: ndarray of shape (nsamples,) the labels used to compute the clustering metrics which requires some supervision \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - t0\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "    \n",
    "    # Define the metrics which require only the true labels and estimator\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "    \n",
    "    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "    \n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(data, estimator[-1].labels_,\n",
    "                                metric=\"euclidean\", sample_size=300,)\n",
    "    ]\n",
    "    \n",
    "    # Show the results\n",
    "    formatter_results = (\"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\"\n",
    "                        \"\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\")\n",
    "    print(formatter_results.format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette\n",
      "2        \t0.008s\t2024\t0.156\t0.886\t0.266\t0.021\t0.091\t0.282\n",
      "3        \t0.007s\t1725\t0.246\t0.944\t0.391\t0.037\t0.137\t0.077\n",
      "4        \t0.007s\t1573\t0.282\t0.846\t0.422\t0.041\t0.128\t0.041\n",
      "5        \t0.004s\t1487\t0.336\t0.879\t0.486\t0.057\t0.156\t0.053\n",
      "6        \t0.005s\t1318\t0.351\t0.818\t0.491\t0.057\t0.137\t-0.059\n",
      "7        \t0.005s\t1225\t0.401\t0.849\t0.545\t0.076\t0.165\t-0.075\n",
      "8        \t0.004s\t1113\t0.435\t0.849\t0.576\t0.090\t0.174\t-0.093\n",
      "9        \t0.005s\t1042\t0.470\t0.866\t0.609\t0.106\t0.194\t-0.149\n",
      "10       \t0.005s\t1024\t0.497\t0.878\t0.635\t0.121\t0.211\t-0.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "for n_clusters in range(2, 11):\n",
    "\n",
    "    pca = PCA(n_components=n_clusters).fit(data_vg2)\n",
    "    kmeans = KMeans(init=pca.components_, n_clusters=n_clusters, n_init=1)\n",
    "    bench_kmeans(kmeans=kmeans, name=\"{}\".format(n_clusters), data=data_vg2, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
